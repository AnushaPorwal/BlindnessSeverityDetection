{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNqsQfPW7QhMj4hXr3yGbCE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Blindness Severity Detection using EfficientNet models\n","Starting Instructions:\n","\n","1.   Download original data and place it in the basepath directory (along with this colab notebook)\n","2.   Rename the folder as data_asDownloaded\n","3.   Load the data_subset and data_augmented zip into the same folder.\n","4. There are few more instructions on modifying the data_asDownloaded/ folder ahead (in the next section)\n","5. If you wanto to load the best model checkpoint for each model, it is present in the best model folder. BestModels.zip needs to be placed in the same folder and unzipped. While loading the best models in each section, you can uncomment the line that loads the best models from the BestModels folder, instead of picking the one from the checkpoint folder. (This is incase you don't want to train everything again)\n","\n","Folder content should look like:\n","```\n","/content/drive/MyDrive/AI4H_project\n","├── ProjectCode_colab.ipynb\n","├── BestModels.zip\n","│   ├── b0_linear/model_checkpoint_16.pt\n","│   ├── efficientnet_b4_dup_augData/model_checkpoint_17.pt\n","│   ├── efficientnet_b4_dup_subsetData/model_checkpoint_37.pt\n","│   ├── efficientnet_b4_randomweight/model_checkpoint_17.pt\n","├── data_subset.zip\n","│   ├── train/\n","│       ├── 0/\n","│       ├── 1/\n","│       ├── 2/\n","│       ├── ...\n","├── data_augmented.zip\n","│   ├── train/\n","│       ├── 0/\n","│       ├── 1/\n","│       ├── 2/\n","│       ├── ...\n","└── data_asDownloaded/\n","│   ├── train.csv\n","│   ├── test.csv\n","│   ├── valid.csv\n","│   ├── train_images/\n","│       ├── <img1>.png/\n","│       ├── <img2>.png/\n","│       ├── <img3>.png/\n","│       ├── ...\n","│   ├── test_images/\n","│       ├── <img1>.png/\n","│       ├── <img2>.png/\n","│       ├── <img3>.png/\n","│       ├── ...\n","│   ├── valid_images/\n","│       ├── <img1>.png/\n","│       ├── <img2>.png/\n","│       ├── <img3>.png/\n","│       ├── ...\n","```\n","\n","Code References used:\n","\n","1.   Data Augmentation: https://github.com/enrico310786/brain_tumor_classification/blob/master/augment_train_dataset.py\n","2.   EfficientNet_b5:\n","*    https://github.com/enrico310786/brain_tumor_classification/tree/master/config\n","* https://medium.com/@enrico.randellini/image-classification-resnet-vs-efficientnet-vs-efficientnet-v2-vs-compact-convolutional-c205838bbf49\n","\n","3. Image Preprocessing: https://www.kaggle.com/code/carlolepelaars/efficientnetb5-with-keras-aptos-2019#Preprocessing-\n","\n","4. Random Weighted Sampler:\n","* https://pytorch.org/docs/stable/data.html\n","* https://stackoverflow.com/questions/69318733/intution-behind-weighted-random-sampler-in-pytorch\n","* https://discuss.pytorch.org/t/proper-way-of-using-weightedrandomsampler/73147\n","* https://discuss.pytorch.org/t/how-to-handle-imbalanced-classes/11264/11\n","* https://discuss.pytorch.org/t/how-to-prevent-overfitting/1902/3?u=smth\n","\n","5. Model Training and general architecture:\n","* https://discuss.pytorch.org/t/load-only-a-part-of-the-network-with-pretrained-weights/88397/2\n","* https://www.kaggle.com/code/veb101/transfer-learning-using-efficientnet-models\n","* https://www.kaggle.com/code/carlolepelaars/efficientnetb5-with-keras-aptos-2019#Preparation-\n","* https://discuss.pytorch.org/t/add-layers-on-pretrained-model/88760/3\n","* https://debuggercafe.com/transfer-learning-using-efficientnet-pytorch/\n","* https://www.kaggle.com/code/imdadulhaquesourav/diabetic-retinopathy-efficientnetb0-b4-b5-resnet\n","* https://www.kaggle.com/code/arjungsanal/efficent-net-using-aptos\n","* https://www.kaggle.com/code/quddusikashaf/projectbook"],"metadata":{"id":"bO1IVDLmgsh1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"edFYKLUtCgeJ"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","import torch\n","import pathlib\n","import random\n","import cv2\n","import gc\n","import sys\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","import torch.nn.functional as F\n","\n","from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n","from torch import nn\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import datasets, transforms\n","import torchvision.models as models\n","import torch.optim as optim\n","\n","import torch\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import numpy as np\n","import json\n","import time\n","import requests\n","import matplotlib.pyplot as plt\n","import warnings\n","import os\n","import shutil\n","from scipy.stats import ttest_rel\n","warnings.filterwarnings('ignore')\n","\n","device = torch.device(\"cpu\")\n","print(f'Using {device}')"]},{"cell_type":"code","source":["# This will mount your google drive folder\n","from google.colab import drive"],"metadata":{"id":"snNeV-6ECoqp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","!mkdir /content/drive/MyDrive/AI4H_project\n","!ls /content/drive/MyDrive/AI4H_project"],"metadata":{"id":"WBdda23nCwti"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creating class folders for the data and copying images into it\n","\n","1.   Rename val csv to valid.csv\n","2.   Rename val_images folder to valid_images (for easy copying into class folders)\n","3.   Rename train_1.csv to train.csv (for easy copying into class folders)"],"metadata":{"id":"1acxTo5hDvnH"}},{"cell_type":"code","source":["subset = ['train', 'test', 'valid']"],"metadata":{"id":"dGqim2Y1CyYE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_path='/content/drive/MyDrive/AI4H_project'"],"metadata":{"id":"FTl1qbJoDZ-v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.mkdir(base_path + '/data')\n","os.mkdir(base_path + '/data/train')\n","os.mkdir(base_path + '/data/test')\n","os.mkdir(base_path + '/data/valid')\n","\n","os.mkdir(base_path + '/outputs')"],"metadata":{"id":"VY7dugSbDzyp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for s in subset:\n","    for i in range(5):\n","        os.makedirs(base_path + '/data/' + s + '/'+str(i), exist_ok=True)"],"metadata":{"id":"K4ZPaIIqD1Ux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for s in subset:\n","    data = pd.read_csv(base_path + \"/data_asDownloaded/\" + s + \".csv\")\n","    for idx, row in data.iterrows():\n","        category = row['diagnosis']\n","        shutil.copy(base_path + \"/data_asDownloaded\" + \"/\" + s + \"_images/\" + s + \"_images/\" + row['id_code'] + \".png\",\n","                    base_path + \"/data/\" + s + \"/\" + str(row['diagnosis']) + \"/\" + row['id_code'] + \".png\")"],"metadata":{"id":"7wYabUr9D-2N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The basic data folder is now ready. It is now in the format so that the DataLoader can load it (the images are now subset-wise and class-wise)."],"metadata":{"id":"yryHhDeFEXzO"}},{"cell_type":"markdown","source":["### Data for Model 1: Baseline\n","Unzip the data_subset file into a folder called data_subset. This data will be used for the baseline model.\n","It contains the same number of examples for each class. The samples are picked randomly."],"metadata":{"id":"S8pUuarUFCi3"}},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/AI4H_project/data_subset.zip -d /content/drive/MyDrive/AI4H_project/"],"metadata":{"id":"vx6a2f9FE6CN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data for Model 4: Augmented Data\n","Unzip the data_augmented zip file into a folder called data_augmented. This data will be used for Model 4. It contains the same number of examples for each class, about 1600 samples per class."],"metadata":{"id":"9PquG1rfV4C_"}},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/AI4H_project/data_augmented.zip -d /content/drive/MyDrive/AI4H_project/"],"metadata":{"id":"ntHcoehBWPoL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Best Models from previous training\n","Unzip the data_augmented zip file into a folder called data_augmented"],"metadata":{"id":"tk1wmlTnn5aA"}},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/AI4H_project/BestModels.zip -d /content/drive/MyDrive/AI4H_project/"],"metadata":{"id":"5UYs4Z1aoA0i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Functions that are used by all models"],"metadata":{"id":"zGN_n2cAHU-3"}},{"cell_type":"code","source":["def get_data_loaders(dataset_train, dataset_valid, dataset_test_valTF, dataset_test_trainTF, BATCH_SIZE=8):\n","    train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n","    valid_loader = DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader_valTF  = DataLoader(dataset_test_valTF, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader_trainTF  = DataLoader(dataset_test_trainTF, batch_size=BATCH_SIZE, shuffle=False)\n","    return train_loader, valid_loader, test_loader_valTF, test_loader_trainTF"],"metadata":{"id":"CJDT8avwPgza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss function:\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"qlnERbM1HeyT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, trainloader, optimizer, criterion, epochNum, modelName_folder):\n","    model.train()\n","    print('Training')\n","    train_running_loss = 0.0\n","    train_running_correct = 0\n","    counter = 0\n","    list_preds = []\n","    list_labels = []\n","    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n","        counter += 1\n","        image, labels = data\n","        image = image.to(device)\n","        labels = labels.to(device)\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(image)\n","\n","        # Loss.\n","        loss = criterion(outputs, labels)\n","        train_running_loss += loss.item()\n","\n","        # Accuracy\n","        _, preds = torch.max(outputs.data, 1)\n","        train_running_correct += (preds == labels).sum().item()\n","        list_preds.append(preds)\n","        list_labels.append(labels)\n","\n","        # Backpropagation\n","        loss.backward()\n","\n","        # Update the weights:\n","        optimizer.step()\n","\n","    torch.save(model, base_path+f\"/{modelName_folder}/model_checkpoint_{epochNum}.pt\")\n","\n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = train_running_loss / counter\n","    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n","\n","    #for balanced accuracy\n","    list_true = []\n","    for i in range(len(list_labels)):\n","        for j in range(0, list_labels[i].shape[0]):\n","            list_true.append(int(list_labels[i][j]))\n","\n","    list_pred = []\n","    for i in range(len(list_preds)):\n","        for j in range(0, list_preds[i].shape[0]):\n","            list_pred.append(int(list_preds[i][j]))\n","\n","    balanced_acc = balanced_accuracy_score(list_true, list_pred)\n","\n","    return epoch_loss, epoch_acc, balanced_acc"],"metadata":{"id":"NhPbMYq4Heu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validate(model, testloader, criterion):\n","    model.eval()\n","    print('Validation')\n","    valid_running_loss = 0.0\n","    valid_running_correct = 0\n","    counter = 0\n","    list_preds = []\n","    list_labels = []\n","    with torch.no_grad():\n","        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n","            counter += 1\n","\n","            image, labels = data\n","            image = image.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass.\n","            outputs = model(image)\n","\n","            # Calculate the loss.\n","            loss = criterion(outputs, labels)\n","            valid_running_loss += loss.item()\n","\n","            # Calculate the accuracy.\n","            _, preds = torch.max(outputs.data, 1)\n","            valid_running_correct += (preds == labels).sum().item()\n","            list_preds.append(preds)\n","            list_labels.append(labels)\n","\n","    # Loss and accuracy for the complete epoch.\n","    epoch_loss = valid_running_loss / counter\n","    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n","\n","    #for balanced accuracy\n","    list_true = []\n","    for i in range(len(list_labels)):\n","        for j in range(0, list_labels[i].shape[0]):\n","            list_true.append(int(list_labels[i][j]))\n","\n","    list_pred = []\n","    for i in range(len(list_preds)):\n","        for j in range(0, list_preds[i].shape[0]):\n","            list_pred.append(int(list_preds[i][j]))\n","\n","    balanced_acc = balanced_accuracy_score(list_true, list_pred)\n","    return epoch_loss, epoch_acc, balanced_acc, list_true, list_pred"],"metadata":{"id":"H_G8FytxHes6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_plots(train_acc, valid_acc, train_loss, valid_loss, model_name):\n","    \"\"\"\n","    Function to save the loss and accuracy plots\n","    \"\"\"\n","    # accuracy plots\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(train_acc, color='green', linestyle='-', label='train accuracy')\n","    plt.plot(valid_acc, color='blue', linestyle='-', label='validataion accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.savefig(base_path + f\"/outputs/accuracy_pretrained_{model_name}.png\")\n","\n","    # loss plots\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(train_loss, color='orange', linestyle='-', label='train loss')\n","    plt.plot(valid_loss, color='red', linestyle='-', label='validataion loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.savefig(base_path + f\"/outputs/loss_pretrained_{model_name}.png\")"],"metadata":{"id":"OEDWSqDHIuhw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 1: Baseline Model: efficientnet_b4 + subset Data\n","Model_Identifier: efficientnet_b4_dup_subsetData\n"],"metadata":{"id":"BMsYm1uDEma-"}},{"cell_type":"code","source":["def get_train_transform_1():\n","    train_transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        transforms.RandomRotation(15),\n","        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return train_transform\n","\n","\n","# Validation transforms\n","def get_valid_transform_1():\n","    valid_transform = transforms.Compose([\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return valid_transform"],"metadata":{"id":"FPON3GVVESI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets_1():\n","    dataset_train = datasets.ImageFolder(\n","        base_path+\"/data_subset/train\",\n","        #base_path+\"/data/train\",\n","        transform=(get_train_transform_1())\n","    )\n","    dataset_val = datasets.ImageFolder(\n","        base_path+\"/data/valid\",\n","        transform=(get_valid_transform_1())\n","    )\n","    dataset_test_valTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_valid_transform_1())\n","    )\n","\n","    dataset_test_trainTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_train_transform_1())\n","    )\n","    return dataset_train, dataset_val, dataset_test_valTF, dataset_test_trainTF"],"metadata":{"id":"8idd0j-gFxVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_train_1, dataset_valid_1, dataset_test_valTF_1, dataset_test_trainTF_1 = get_datasets_1()\n","train_loader_1, valid_loader_1, test_loader_valTF_1, test_loader_trainTF_1 = get_data_loaders(dataset_train_1, dataset_valid_1, dataset_test_valTF_1, dataset_test_trainTF_1)"],"metadata":{"id":"2KHkD1IQGLny"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"Pl-Y8GxZPk3D"}},{"cell_type":"code","source":["model_1 = models.efficientnet_b4(pretrained=True)\n","model_1.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","#For no finetuning-> Freezing model weights\n","for params in model_1.parameters():\n","    params.requires_grad = False\n","\n","model_1.classifier = nn.Sequential(nn.Flatten(),\n","                                    nn.Linear(1792, 256),\n","                                    nn.ReLU(),\n","                                    nn.Dropout(0.3),\n","                                    nn.Linear(256, 5))"],"metadata":{"id":"O9_-xCZhGntx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_params_1 = sum(p.numel() for p in model_1.parameters())\n","print(f\"{total_params_1:,} total parameters.\")\n","total_trainable_params_1 = sum(\n","    p.numel() for p in model_1.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params_1:,} training parameters.\")"],"metadata":{"id":"EMA-f3XEHFhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This folder is to save the model checkpoints\n","os.mkdir(base_path + '/efficientnet_b4_dup_subsetData')"],"metadata":{"id":"yNijnqm7JmHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer:\n","optimizer_1 = optim.Adam(model_1.parameters(), lr=0.001)\n","\n","epochs = 30\n","train_loss_1, valid_loss_1 = [], []\n","train_acc_1, valid_acc_1 = [], []\n","\n","# Training Loop\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss, train_epoch_acc, train_bacc = train(model_1, train_loader_1, optimizer_1, criterion, epoch, \"efficientnet_b4_dup_subsetData\")\n","    valid_epoch_loss, valid_epoch_acc, valid_bacc, _, _ = validate(model_1, valid_loader_1, criterion)\n","    train_loss_1.append(train_epoch_loss)\n","    valid_loss_1.append(valid_epoch_loss)\n","    train_acc_1.append(train_epoch_acc)\n","    valid_acc_1.append(valid_epoch_acc)\n","    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}, training BACC: {train_bacc:.3f}\")\n","    print(f\"Valid loss: {valid_epoch_loss:.3f}, valid acc: {valid_epoch_acc:.3f}, valid BACC: {valid_bacc:.3f}\")\n","    print('-'*50)\n","    time.sleep(5)"],"metadata":{"id":"PdnXgmWSHPx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_plots(train_acc_1, valid_acc_1, train_loss_1, valid_loss_1, \"efficientnet_b4_dup_subsetData\")"],"metadata":{"id":"3CSdFsFEJCja"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading the best model\n","\n","Loading model with highest Validation BACC: epoch 38 -> model_checkpoint_37"],"metadata":{"id":"QisRB3D-Kfeu"}},{"cell_type":"code","source":["PATH_1 = base_path+f\"/efficientnet_b4_dup_subsetData/model_checkpoint_{37}.pt\"\n","#PATH_1 = base_path+f\"/BestModels/efficientnet_b4_dup_subsetData/model_checkpoint_{37}.pt\"\n","model_1_best = torch.load(PATH_1, weights_only=False)"],"metadata":{"id":"JHLT-SdpJ84B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_1a, accuracy_1a, balanced_acc_1a, list_true_1a, list_pred_1a = validate(model_1_best, test_loader_valTF_1, criterion)"],"metadata":{"id":"9BYhDfONKktq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_1b, accuracy_1b, balanced_acc_1b, list_true_1b, list_pred_1b = validate(model_1_best, test_loader_trainTF_1, criterion)"],"metadata":{"id":"PHCu1TZJKo_i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cks_1a = cohen_kappa_score(list_true_1a, list_pred_1a)\n","cks_1b = cohen_kappa_score(list_true_1b, list_pred_1b)"],"metadata":{"id":"WbzuoMMpLIly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"With Only Resizing of images\\n Test loss: {loss_1a:.3f}\\n Test acc: {accuracy_1a:.3f}\\n Test BACC: {balanced_acc_1a:.3f}\\n Cohen-Kappa: {cks_1a:.3f}\")\n","print(f\"With Training Image preprocessing\\n Test loss: {loss_1b:.3f}\\n Test acc: {accuracy_1b:.3f}\\n Test BACC: {balanced_acc_1b:.3f}\\n Cohen-Kappa: {cks_1b:.3f}\")"],"metadata":{"id":"s9qGrNM3LKR_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 2: EfficientNet_b0 + Linear Classification Layer\n","model_identifier: b0_linear"],"metadata":{"id":"uqxRIixtLOBj"}},{"cell_type":"code","source":["def get_train_transform_2():\n","    train_transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        transforms.Resize((300, 300)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return train_transform\n","\n","\n","# Validation transforms\n","def get_valid_transform_2():\n","    valid_transform = transforms.Compose([\n","        transforms.Resize((300, 300)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return valid_transform"],"metadata":{"id":"w56qdgNrLL2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets_2():\n","    dataset_train = datasets.ImageFolder(\n","        base_path+\"/data/train\",\n","        transform=(get_train_transform_2())\n","    )\n","    dataset_val = datasets.ImageFolder(\n","        base_path+\"/data/valid\",\n","        transform=(get_valid_transform_2())\n","    )\n","    dataset_test_valTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_valid_transform_2())\n","    )\n","\n","    dataset_test_trainTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_train_transform_2())\n","    )\n","\n","    return dataset_train, dataset_val, dataset_test_valTF, dataset_test_trainTF"],"metadata":{"id":"7D5hJHeaLs5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_train_2, dataset_valid_2, dataset_test_valTF_2, dataset_test_trainTF_2  = get_datasets_2()\n","train_loader_2, valid_loader_2, test_loader_valTF_2, test_loader_trainTF_2 = get_data_loaders(dataset_train_2, dataset_valid_2, dataset_test_valTF_2, dataset_test_trainTF_2)"],"metadata":{"id":"JN-L3QUOMcnQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_2 = models.efficientnet_b0(pretrained=True)\n","\n","for params in model_2.parameters():\n","    params.requires_grad = False\n","\n","model_2.classifier[1] = nn.Linear(in_features=1280, out_features=5)"],"metadata":{"id":"v9G-qhm_MwM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_params_2 = sum(p.numel() for p in model_2.parameters())\n","print(f\"{total_params_2:,} total parameters.\")\n","total_trainable_params_2 = sum(\n","    p.numel() for p in model_2.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params_2:,} training parameters.\")"],"metadata":{"id":"pI1MBsioM_AF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This folder is to save the model checkpoints\n","os.mkdir(base_path + '/b0_linear')"],"metadata":{"id":"21vxrJ8EOayy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer:\n","optimizer_2 = optim.Adam(model_2.parameters(), lr=0.0001)\n","\n","epochs = 20\n","train_loss_2, valid_loss_2 = [], []\n","train_acc_2, valid_acc_2 = [], []\n","\n","# Training Loop\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss, train_epoch_acc, train_bacc = train(model_2, train_loader_2, optimizer_2, criterion, epoch, \"b0_linear\")\n","    valid_epoch_loss, valid_epoch_acc, valid_bacc, _, _ = validate(model_2, valid_loader_2, criterion)\n","    train_loss_2.append(train_epoch_loss)\n","    valid_loss_2.append(valid_epoch_loss)\n","    train_acc_2.append(train_epoch_acc)\n","    valid_acc_2.append(valid_epoch_acc)\n","    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}, training BACC: {train_bacc:.3f}\")\n","    print(f\"Valid loss: {valid_epoch_loss:.3f}, valid acc: {valid_epoch_acc:.3f}, valid BACC: {valid_bacc:.3f}\")\n","    print('-'*50)\n","    time.sleep(5)"],"metadata":{"id":"WCsbun6jNG5c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_plots(train_acc_2, valid_acc_2, train_loss_2, valid_loss_2, \"baseline_1\")"],"metadata":{"id":"xc78xILaNipt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading the best model\n","\n","Loading model with highest Validation BACC: epoch 17 -> model_checkpoint_16"],"metadata":{"id":"sq7F23PXN0T5"}},{"cell_type":"code","source":["PATH_2 = base_path+f\"/b0_linear/model_checkpoint_{16}.pt\"\n","#PATH_2 = base_path+f\"/BestModels/b0_linear/model_checkpoint_{16}.pt\"\n","model_2_best = torch.load(PATH_2, weights_only=False)"],"metadata":{"id":"VOskX_RlNrrR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_2a, accuracy_2a, balanced_acc_2a, list_true_2a, list_pred_2a = validate(model_2_best, test_loader_valTF_2, criterion)"],"metadata":{"id":"KDuAfJn2N_l7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_2b, accuracy_2b, balanced_acc_2b, list_true_2b, list_pred_2b = validate(model_2_best, test_loader_trainTF_2, criterion)"],"metadata":{"id":"WW0ExCKQOCPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cks_2a = cohen_kappa_score(list_true_2a, list_pred_2a)\n","cks_2b = cohen_kappa_score(list_true_2b, list_pred_2b)"],"metadata":{"id":"dTACMT1oOHbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"With Only Resizing of images\\n Test loss: {loss_2a:.3f}\\n Test acc: {accuracy_2a:.3f}\\n Test BACC: {balanced_acc_2a:.3f}\\n Cohen-Kappa: {cks_2a:.3f}\")\n","print(f\"With Training Image preprocessing\\n Test loss: {loss_2b:.3f}\\n Test acc: {accuracy_2b:.3f}\\n Test BACC: {balanced_acc_2b:.3f}\\n Cohen-Kappa: {cks_2b:.3f}\")"],"metadata":{"id":"T3_7az7vOJ_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 3: RandomWeightSampler + EfficientNet_b4\n","Model_identifier: efficientnet_b4_randomweight"],"metadata":{"id":"fGCvz248O01e"}},{"cell_type":"code","source":["def get_train_transform_3():\n","    train_transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        transforms.RandomRotation(15),\n","        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return train_transform\n","\n","\n","def get_valid_transform_3():\n","    valid_transform = transforms.Compose([\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return valid_transform"],"metadata":{"id":"d_SdYMRpOLnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["traindata = pd.read_csv(base_path + \"/data_asDownloaded/train.csv\")\n","dg_list = traindata['diagnosis'].to_list()\n","freq = Counter(dg_list)\n","freq"],"metadata":{"id":"yvCT3IaNYm9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_counts = np.array([1434, 300, 808, 154, 234])\n","weight = 1. / class_counts\n","weight"],"metadata":{"id":"oBPSO8bOY1PC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_weights = np.array(weight[traindata['diagnosis'].to_list()])\n","sample_weights"],"metadata":{"id":"sSQwsyFMY3j9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sampler = torch.utils.data.WeightedRandomSampler(sample_weights, len(traindata['diagnosis'].to_list()))"],"metadata":{"id":"HjW-D1fCY8yQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets_3():\n","    dataset_train = datasets.ImageFolder(\n","        base_path+\"/data/train\",\n","        transform=(get_train_transform_3())\n","    )\n","    dataset_val = datasets.ImageFolder(\n","        base_path+\"/data/valid\",\n","        transform=(get_valid_transform_3())\n","    )\n","    dataset_test_valTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_valid_transform_3())\n","    )\n","\n","    dataset_test_trainTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_train_transform_3())\n","    )\n","\n","    return dataset_train, dataset_val, dataset_test_valTF, dataset_test_trainTF\n","\n","def get_data_loaders_3(dataset_train, dataset_valid, dataset_test_valTF, dataset_test_trainTF, BATCH_SIZE=8):\n","    train_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE, sampler=sampler)\n","    valid_loader = DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader_valTF_3 = DataLoader(dataset_test_valTF, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader_trainTF_3 = DataLoader(dataset_test_trainTF, batch_size=BATCH_SIZE, shuffle=False)\n","    return train_loader, valid_loader, test_loader_valTF_3, test_loader_trainTF_3"],"metadata":{"id":"K17aP7fwPCNq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_train_3, dataset_valid_3, dataset_test_valTF_3, dataset_test_trainTF_3 = get_datasets_3()\n","train_loader_3, valid_loader_3, test_loader_valTF_3, test_loader_trainTF_3 = get_data_loaders_3(dataset_train_3, dataset_valid_3, dataset_test_valTF_3, dataset_test_trainTF_3)"],"metadata":{"id":"yVpMTNCvPuot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_3 = models.efficientnet_b4(pretrained=True)\n","model_3.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","#For no finetuning-> Freezing model weights\n","for params in model_3.parameters():\n","    params.requires_grad = False\n","\n","model_3.classifier = nn.Sequential(nn.Flatten(),\n","                                    nn.Linear(1792, 256),\n","                                    nn.ReLU(),\n","                                    nn.Dropout(0.3),\n","                                    nn.Linear(256, 5))"],"metadata":{"id":"59RguDLWP-6T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_params_3 = sum(p.numel() for p in model_3.parameters())\n","print(f\"{total_params_3:,} total parameters.\")\n","total_trainable_params_3 = sum(\n","    p.numel() for p in model_3.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params_3:,} training parameters.\")"],"metadata":{"id":"JCs3d9__QLnF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This folder is to save the model checkpoints\n","os.mkdir(base_path + '/efficientnet_b4_randomweight')"],"metadata":{"id":"XaHoRl56QWXJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer:\n","optimizer_3 = optim.Adam(model_3.parameters(), lr=0.001)\n","\n","epochs = 20\n","\n","train_loss_3, valid_loss_3 = [], []\n","train_acc_3, valid_acc_3 = [], []\n","\n","# Training Loop\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss, train_epoch_acc, train_bacc = train(model_3, train_loader_3, optimizer_3, criterion, epoch, \"efficientnet_b4_randomweight\")\n","    valid_epoch_loss, valid_epoch_acc, valid_bacc, _, _ = validate(model_3, valid_loader_3, criterion)\n","    train_loss_3.append(train_epoch_loss)\n","    valid_loss_3.append(valid_epoch_loss)\n","    train_acc_3.append(train_epoch_acc)\n","    valid_acc_3.append(valid_epoch_acc)\n","    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}, training BACC: {train_bacc:.3f}\")\n","    print(f\"Valid loss: {valid_epoch_loss:.3f}, valid acc: {valid_epoch_acc:.3f}, valid BACC: {valid_bacc:.3f}\")\n","    print('-'*50)\n","    time.sleep(5)"],"metadata":{"id":"O5amyscJQTRG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading the best model\n","\n","Loading model with highest Validation BACC: epoch 18 -> model_checkpoint_17"],"metadata":{"id":"yc9Ffuz2Q4t0"}},{"cell_type":"code","source":["PATH_3 = base_path+f\"/efficientnet_b4_randomweight/model_checkpoint_{17}.pt\"\n","#PATH_3 = base_path+f\"/BestModels/efficientnet_b4_randomweight/model_checkpoint_{17}.pt\"\n","model_3_best = torch.load(PATH_3, weights_only=False)"],"metadata":{"id":"nIzznddGQzsN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_3a, accuracy_3a, balanced_acc_3a, list_true_3a, list_pred_3a = validate(model_3_best, test_loader_valTF_3, criterion)\n","loss_3b, accuracy_3b, balanced_acc_3b, list_true_3b, list_pred_3b = validate(model_3_best, test_loader_trainTF_3, criterion)"],"metadata":{"id":"Oz2DiWBERchu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cks_3a = cohen_kappa_score(list_true_3a, list_pred_3a)\n","cks_3b = cohen_kappa_score(list_true_3b, list_pred_3b)"],"metadata":{"id":"QX003HbQRg0M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"With Only Resizing of images\\n Test loss: {loss_3a:.3f}\\n Test acc: {accuracy_3a:.3f}\\n Test BACC: {balanced_acc_3a:.3f}\\n Kappa-Cohen: {cks_3a:.3f}\")\n","print(f\"With Training Image preprocessing\\n Test loss: {loss_3b:.3f}\\n Test acc: {accuracy_3b:.3f}\\n Test BACC: {balanced_acc_3b:.3f}\\n Kappa-Cohen: {cks_3b:.3f}\")"],"metadata":{"id":"2DLb3e5tRipF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model 4: Data Augmentation + EfficientNet_b4\n","Model_identifier: efficientnet_b4_dup_augData"],"metadata":{"id":"C-igU2hrRngd"}},{"cell_type":"code","source":["def get_train_transform_4():\n","    train_transform = transforms.Compose([\n","        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n","        transforms.ColorJitter(brightness=(1.4, 1.6), contrast=(1.5,2)),\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return train_transform\n","\n","def get_valid_transform_4():\n","    valid_transform = transforms.Compose([\n","        transforms.Resize((380, 380)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","          mean=[0.485, 0.456, 0.406],\n","          std=[0.229, 0.224, 0.225]\n","        )\n","    ])\n","    return valid_transform"],"metadata":{"id":"dN_ftw89Rm9d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_datasets_4():\n","    dataset_train = datasets.ImageFolder(\n","        base_path+\"/data_augmented/train_augmented\",\n","        transform=(get_train_transform_4())\n","    )\n","    dataset_val = datasets.ImageFolder(\n","        base_path+\"/data/valid\",\n","        transform=(get_valid_transform_4())\n","    )\n","    dataset_test_valTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_valid_transform_4())\n","    )\n","    dataset_test_trainTF = datasets.ImageFolder(\n","        base_path+\"/data/test\",\n","        transform=(get_train_transform_4())\n","    )\n","    return dataset_train, dataset_val, dataset_test_valTF, dataset_test_trainTF"],"metadata":{"id":"cbPhzUYPSvJ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_train_4, dataset_valid_4, dataset_test_valTF_4, dataset_test_trainTF_4  = get_datasets_4()\n","train_loader_4, valid_loader_4, test_loader_valTF_4, test_loader_trainTF_4 = get_data_loaders(dataset_train_4, dataset_valid_4, dataset_test_valTF_4, dataset_test_trainTF_4)"],"metadata":{"id":"F_77dZW9Tg5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_4 = models.efficientnet_b4(pretrained=True)\n","model_4.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n","\n","#For no finetuning-> Freezing model weights\n","for params in model_4.parameters():\n","    params.requires_grad = False\n","\n","model_4.classifier = nn.Sequential(nn.Flatten(),\n","                                    nn.Linear(1792, 256),\n","                                    nn.ReLU(),\n","                                    nn.Dropout(0.3),\n","                                    nn.Linear(256, 5))"],"metadata":{"id":"NRvLGlvGT9Om"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_params_4 = sum(p.numel() for p in model_4.parameters())\n","print(f\"{total_params_4:,} total parameters.\")\n","total_trainable_params_4 = sum(\n","    p.numel() for p in model_4.parameters() if p.requires_grad)\n","print(f\"{total_trainable_params_4:,} training parameters.\")"],"metadata":{"id":"lLzqkOIFUE5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This is where the model checkpoints will be saved\n","os.mkdir(base_path + '/efficientnet_b4_dup_augData')"],"metadata":{"id":"XkdpTZwBUPl6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer:\n","optimizer_4 = optim.Adam(model_4.parameters(), lr=0.001)\n","\n","epochs = 20\n","\n","train_loss_4, valid_loss_4 = [], []\n","train_acc_4, valid_acc_4 = [], []\n","\n","# Training Loop\n","for epoch in range(epochs):\n","    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n","    train_epoch_loss, train_epoch_acc, train_bacc = train(model_4, train_loader_4, optimizer_4, criterion, epoch, \"efficientnet_b4_dup_augData\")\n","    valid_epoch_loss, valid_epoch_acc, valid_bacc, _, _ = validate(model_4, valid_loader_4, criterion)\n","    train_loss_4.append(train_epoch_loss)\n","    valid_loss_4.append(valid_epoch_loss)\n","    train_acc_4.append(train_epoch_acc)\n","    valid_acc_4.append(valid_epoch_acc)\n","    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}, training BACC: {train_bacc:.3f}\")\n","    print(f\"Valid loss: {valid_epoch_loss:.3f}, valid acc: {valid_epoch_acc:.3f}, valid BACC: {valid_bacc:.3f}\")\n","    print('-'*50)\n","    time.sleep(5)"],"metadata":{"id":"bnY8yXG2ULXC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Loading the best model\n","\n","Loading model with highest Validation BACC: epoch 18 -> model_checkpoint_17"],"metadata":{"id":"8Ae90lzvU3Ea"}},{"cell_type":"code","source":["PATH_4 = base_path+f\"/efficientnet_b4_dup_augData/model_checkpoint_{17}.pt\"\n","#PATH_4 = base_path+f\"BestModels/efficientnet_b4_dup_augData/model_checkpoint_{17}.pt\"\n","model_4_best = torch.load(PATH_4, weights_only=False)"],"metadata":{"id":"CiStZ1CvUwka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_4a, accuracy_4a, balanced_acc_4a, list_true_4a, list_pred_4a = validate(model_4_best, test_loader_valTF_4, criterion)\n","loss_4b, accuracy_4b, balanced_acc_4b, list_true_4b, list_pred_4b = validate(model_4_best, test_loader_trainTF_4, criterion)"],"metadata":{"id":"Q1Ns09gAU8HW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cks_4a = cohen_kappa_score(list_true_4a, list_pred_4a)\n","cks_4b = cohen_kappa_score(list_true_4b, list_pred_4b)"],"metadata":{"id":"ppOQDUa1VFWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"With Only Resizing of images\\n Test loss: {loss_4a:.3f}\\n Test acc: {accuracy_4a:.3f}\\n Test BACC: {balanced_acc_4a:.3f}\\n Cohen-Kappa: {cks_4a:.3f}\")\n","print(f\"With Training Image preprocessing\\n Test loss: {loss_4b:.3f}\\n Test acc: {accuracy_4b:.3f}\\n Test BACC: {balanced_acc_4b:.3f}\\n Cohen-Kappa: {cks_4b:.3f}\")"],"metadata":{"id":"9ZzNfHsRVGxo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hypothesis testing\n"],"metadata":{"id":"ezzFXug9YEQI"}},{"cell_type":"code","source":["ttest_rel([accuracy_1a, balanced_acc_1a, cks_1a], [accuracy_3a, balanced_acc_3a, cks_3a]).pvalue"],"metadata":{"id":"9dRV4GXQVI6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QavxxahMYW3T"},"execution_count":null,"outputs":[]}]}